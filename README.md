#  Text Classification with Extremely Small and Unbalanced Datasets

Pre-trained NLP classifiers' performance drops significantly when fine-tuning with small and unbalanced training dataset. In this project, I presented a series of experiments using a variety of approaches, such as GAN-BERT, Sentence-BERT, Few-shot learning... etc. The experiment showed that classifiers' performance boosted by 5-10%, comparing to original BERT model.
